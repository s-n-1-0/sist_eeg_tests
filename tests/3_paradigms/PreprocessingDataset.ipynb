{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import mne\n",
    "from eeghdf import EEGHDFUpdater\n",
    "root_path = \"//172.16.88.200/private/2221012/MIOnly_FTP_EEG Dataset and OpenBMI Toolbox for Three BCI Paradigms\"\n",
    "file_paths = glob.glob(root_path + \"/pres/*.set\")\n",
    "fs = 500\n",
    "ch = 14 #C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "finfo_list = []\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    fns = file_name.split(\"_\")\n",
    "    session = int(fns[0][4:])\n",
    "    subject = int(fns[1][4:])\n",
    "    finfo_list.append((file_path,session,subject))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abfcf59e",
   "metadata": {},
   "source": [
    "'Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'T7', 'C3', 'Cz', 'C4', 'T8', 'TP9', 'CP5', 'CP1', 'CP2', 'CP6', 'TP10', 'P7', 'P3', 'Pz', 'P4', 'P8', 'PO9', 'O1', 'Oz', 'O2', 'PO10', 'FC3', 'FC4', 'C5', 'C1', 'C2', 'C6', 'CP3', 'CPz', 'CP4', 'P1', 'P2', 'POz', 'FT9', 'FTT9h', 'TTP7h', 'TP7', 'TPP9h', 'FT10', 'FTT10h', 'TPP8h', 'TP8', 'TPP10h', 'F9', 'F10', 'AF7', 'AF3', 'AF4', 'AF8', 'PO3', 'PO4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeglist = ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'T7', 'C3', 'Cz', 'C4', 'T8', 'TP9', 'CP5', 'CP1', 'CP2', 'CP6', 'TP10', 'P7', 'P3', 'Pz', 'P4', 'P8', 'PO9', 'O1', 'Oz', 'O2', 'PO10', 'FC3', 'FC4', 'C5', 'C1', 'C2', 'C6', 'CP3', 'CPz', 'CP4', 'P1', 'P2', 'POz', 'FT9', 'FTT9h', 'TTP7h', 'TP7', 'TPP9h', 'FT10', 'FTT10h', 'TPP8h', 'TP8', 'TPP10h', 'F9', 'F10', 'AF7', 'AF3', 'AF4', 'AF8', 'PO3', 'PO4']\n",
    "using_lst = ['FC5','FC1','FC2','FC6','C3','C1','Cz','C2','C4','CP5','CP1','CP2','CP6']\n",
    "#using_lst = ['C3','Cz','C4']\n",
    "ch_indexes = []\n",
    "for item in using_lst:\n",
    "    ch_indexes.append(eeglist.index(item))\n",
    "\n",
    "len(ch_indexes),ch_indexes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f46b84a",
   "metadata": {},
   "source": [
    "## Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c914e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preview_path,session,subject, = finfo_list[6]\n",
    "x = mne.io.read_epochs_eeglab(preview_path).get_data(item=[\"left\",\"right\"])\n",
    "x.shape,session,subject"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a0317ea",
   "metadata": {},
   "source": [
    "### 標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = x[0,:,:]\n",
    "dx = StandardScaler().fit_transform(dx.T).T\n",
    "#test: np.array([[1,2,3],[10,20,30]]),StandardScaler().fit_transform(np.array([[1,2,3],[10,20,30]]).T).T\n",
    "dx.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b2a40a8",
   "metadata": {},
   "source": [
    "### パワースペクトルに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パワースペクトルへの変換\n",
    "freq, px = signal.periodogram(dx[:,500:1000], fs=fs)\n",
    "trimmed_freq_flags = (freq >= 8) & (freq <= 20)\n",
    "print(len(freq[trimmed_freq_flags]),np.where(trimmed_freq_flags)[0][0],np.where(trimmed_freq_flags)[0][-1])\n",
    "plt.plot(freq[trimmed_freq_flags],np.mean(px,axis=0)[trimmed_freq_flags])\n",
    "plt.ylim((0,0.050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405f662",
   "metadata": {},
   "source": [
    "### DWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "def dwt(ch_signal):\n",
    "    wavelet = 'db4'  # Daubechies 4 wavelet\n",
    "    level = pywt.dwt_max_level(len(ch_signal), wavelet)  # maximum feasible level\n",
    "    coefficients = pywt.wavedec(ch_signal, wavelet, level=level)\n",
    "    return coefficients[4:6]\n",
    "w = []\n",
    "for i in range(dx.shape[0]):\n",
    "    ch_dx = dx[i,500:1000]\n",
    "    _w = dwt(ch_dx)\n",
    "    _w = [np.concatenate([__w, np.zeros(len(ch_dx) - len(__w))]) for __w in _w]\n",
    "    w.append(_w)\n",
    "w = np.array(w)\n",
    "print(w.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55db915a",
   "metadata": {},
   "source": [
    "## まとめて処理(trainのみ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = EEGHDFUpdater(hdf_path=root_path+\"/3pdataset.h5\",\n",
    "                        fs=fs,\n",
    "                        lables=[\"left\",\"right\"])\n",
    "updater.remove_hdf()\n",
    "for path,session,subject in finfo_list:\n",
    "    updater.add_eeglab(path,{\"session\":int(session),\"subject\":int(subject)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "def bandpass(data):\n",
    "    lowcut = 8  # バンドパスフィルタの下限周波数\n",
    "    highcut = 30  # バンドパスフィルタの上限周波数\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(2, [low, high], btype='band')\n",
    "    data = filtfilt(b, a, data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051bfb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = EEGHDFUpdater(hdf_path=root_path+\"/3pdataset.h5\",\n",
    "                        fs=fs,\n",
    "                        lables=[\"left\",\"right\"])\n",
    "#std\n",
    "def prepro_func(x:np.ndarray):\n",
    "    x = bandpass(x)\n",
    "    return StandardScaler().fit_transform(x.T).T #標準化\n",
    "updater.preprocess(\"std\",prepro_func)\n",
    "\n",
    "#psd\n",
    "def prepro_func(x:np.ndarray):\n",
    "    x = bandpass(x)\n",
    "    x = StandardScaler().fit_transform(x.T).T\n",
    "    _, px = signal.periodogram(x[:,500:1000], fs=fs)\n",
    "    return px\n",
    "updater.preprocess(\"psd\",prepro_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8bbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "updater = EEGHDFUpdater(hdf_path=root_path+\"/3pdataset.h5\",\n",
    "                        fs=fs,\n",
    "                        lables=[\"left\",\"right\"])\n",
    "def dwt(ch_signal):\n",
    "    wavelet = 'db4'  # Daubechies 4 wavelet\n",
    "    level = pywt.dwt_max_level(len(ch_signal), wavelet)  # maximum feasible level\n",
    "    coefficients = pywt.wavedec(ch_signal, wavelet, level=level)\n",
    "    return coefficients[4:6]\n",
    "def prepro_func(x:np.ndarray):\n",
    "    x = bandpass(x)\n",
    "    x = StandardScaler().fit_transform(x.T).T\n",
    "    w = []\n",
    "    for i in range(dx.shape[0]):\n",
    "        ch_dx = x[i,500:1000]\n",
    "        _w = dwt(ch_dx)\n",
    "        _w = [np.concatenate([__w, np.zeros(len(ch_dx) - len(__w))]) for __w in _w]\n",
    "        w.append(_w)\n",
    "    wx = np.array(w)\n",
    "    return wx\n",
    "updater.preprocess(\"dwt\",prepro_func)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cec5d90",
   "metadata": {},
   "source": [
    "## restデータ統合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import signal\n",
    "def downsample_multichannel_signal(input_signal, original_rate, target_rate):\n",
    "    num_channels = input_signal.shape[0]  # チャネル数\n",
    "    num_samples = input_signal.shape[1]  # サンプル数\n",
    "    # ダウンサンプリング後のサンプル数を計算\n",
    "    target_samples = int(num_samples * (target_rate / original_rate))\n",
    "    # 出力信号の配列を作成\n",
    "    downsampled_signal = np.zeros((num_channels, target_samples))\n",
    "    for channel in range(num_channels):\n",
    "        # ダウンサンプリングするチャネルの信号を取得\n",
    "        channel_signal = input_signal[channel, :]\n",
    "        # ダウンサンプリング\n",
    "        downsampled_channel_signal = signal.resample(channel_signal, target_samples)\n",
    "        # 出力信号に格納\n",
    "        downsampled_signal[channel, :] = downsampled_channel_signal\n",
    "    return downsampled_signal\n",
    "dataset_size = (54,54)\n",
    "train_key = \"EEG_MI_train\"\n",
    "def parse_mat(parent_key,mat_data):\n",
    "    data = mat_data[parent_key][0][0]\n",
    "    #形式整理\n",
    "    for i in range(len(data)):\n",
    "        sq_data = data[i].squeeze()\n",
    "        if len(sq_data.shape) == 1:\n",
    "            data[i] =  sq_data\n",
    "        if len(sq_data.shape) == 0:\n",
    "            data[i] = sq_data\n",
    "    return data\n",
    "with h5py.File(root_path+\"/3pdataset.h5\",mode=\"r+\") as h5:\n",
    "    if \"rest\" in h5:\n",
    "        del h5[\"rest\"]\n",
    "    rest_group = h5.require_group(\"rest\")\n",
    "    for i, size in enumerate(dataset_size):\n",
    "        for j in range(size):\n",
    "            session = scipy.io.loadmat(f\"{root_path}/session{i+1}/sess{(i+1):02}_subj{(j+1):02}_EEG_MI.mat\")\n",
    "            train_rest = downsample_multichannel_signal(parse_mat(train_key,session)[13].T,1000,500)\n",
    "            train_rest =  StandardScaler().fit_transform(train_rest.T).T\n",
    "            session_group = rest_group.require_group(str(i+1))\n",
    "            session_group.create_dataset(str(j+1),train_rest.shape,data=train_rest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
